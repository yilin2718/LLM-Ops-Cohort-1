{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Reward Model in Reinforcement Learning from Human Feedback (RLHF)\n",
    "\n",
    "The reward model plays a pivotal role in RLHF processes, facilitating a mechanism to gauge and guide a model's real-world behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## Definition:\n",
    "\n",
    "A **reward model** is a specialized component of a machine learning system dedicated to appraising the behavior of a model based on real-world interactions.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow:\n",
    "\n",
    "1. **Input and Output**:\n",
    "   - The reward model processes a *prompt and response pair*.\n",
    "   - It subsequently outputs a corresponding reward or score.\n",
    "   \n",
    "2. **Training Methodology**:\n",
    "   - Human evaluators provide the model with feedback. They review multiple outputs generated by the model and rate them based on quality.\n",
    "   - This feedback equips the reward model with insights, enabling it to accurately assess the primary model's performance.\n",
    "   \n",
    "3. **Integration with the Main Model**:\n",
    "   - The primary (or main) model assimilates this feedback, refining its approach and enhancing performance for forthcoming tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Characteristics:\n",
    "\n",
    "- **Architecture**: The reward model can manifest as an end-to-end language model or even a modular system.\n",
    "  \n",
    "- **Functionality**: Its essential function is to convert input text sequences into a scalar value. This scalar, known as the reward, serves as a crucial bridge for integrating existing reinforcement learning algorithms, ensuring a smooth RLHF workflow.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward Model based on BERT-BASE-UNCASED\n",
    "\n",
    "Dataset: CarperAI/openai_summarize_comparisons "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install transformers\n",
    "!pip install trl\n",
    "!pip install peft\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import random \n",
    "import numpy as np \n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments\n",
    "from trl import RewardTrainer #, RewardConfig\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n",
    "\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Choose BERT as a Rewards Model?\n",
    "\n",
    "BERT, or Bidirectional Encoder Representations from Transformers, stands out as a remarkably efficient model for a plethora of NLP tasks. But what makes it especially suited for a Rewards model? Here are the key reasons:\n",
    "\n",
    "\n",
    "#### 1. **Encoder-Only Architecture**:\n",
    "\n",
    "BERT is fundamentally an encoder-only transformer. Unlike models with decoder components that are tailored for generation tasks, BERT's encoder-only design is optimized for understanding and representing input text. This makes it apt for absorbing the nuances of a text and subsequently producing a scalar reward.\n",
    "\n",
    "\n",
    "#### 2. **Lightweight yet Comprehensive**:\n",
    "\n",
    "Though BERT comes in various sizes, even its base version offers a balance between complexity and efficiency. It's designed to be lightweight enough for practical applications but retains the depth required to understand intricate linguistic constructs.\n",
    "\n",
    "\n",
    "#### 3. **Deep Contextual Understanding**:\n",
    "\n",
    "One of BERT's standout features is its bidirectional context absorption. Instead of reading text in a single direction, BERT processes it both ways, ensuring a holistic understanding. This depth of comprehension is paramount for a Rewards model, enabling it to detect subtle details and intricacies in the input text.\n",
    "\n",
    "\n",
    "#### 4. **Output Suitability**:\n",
    "\n",
    "Given its encoder architecture, BERT's outputs are high-dimensional representations of the input text. These embeddings are readily adaptable for a range of tasks, including the generation of scalar rewards. By applying a simple linear layer on top of these embeddings, we can produce meaningful reward values that reflect the quality or desirability of the input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning BERT-BASE-UNCASED as a Rewards Model\n",
    "\n",
    "In this section, we'll walk through the process of initializing and fine-tuning the `BERT-BASE-UNCASED` model for its use as a rewards model in RLHF.\n",
    "\n",
    "---\n",
    "\n",
    "#### Loading the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased') \n",
    "\n",
    "# Load a tokenizer (change the model name as per your requirements)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the Dataset for Model Training\n",
    "\n",
    "- **Loading the Dataset**: The dataset \"openai_summarize_comparisons\" is sourced from the CarperAI repository.\n",
    "\n",
    "- **Tokenization and Length Calculation**: The 'chosen' column of the dataset undergoes tokenization, with the tokenized input encompassing appended special tokens suitable for BERT-like architectures. For every entry in the dataset, we capture the total number of tokens and store this count in the 'lengths' column.\n",
    "\n",
    "- **Determining Maximum Token Length**: This step identifies the entry with the most tokens in our tokenized dataset, which can aid in setting sequence lengths during training or ensuring consistent padding.\n",
    "\n",
    "- **Shuffling and Random Sampling**: All entries in the 'valid2' subset of our dataset are shuffled randomly. We then derive a subset of this shuffled dataset, based on a predetermined sample size of `n_samples`.\n",
    "\n",
    "- **Size Verification of Processed Dataset**: A final verification is done to confirm the number of entries in our dataset post all the preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/juan_/.cache/huggingface/datasets/CarperAI___parquet/CarperAI--openai_summarize_comparisons-79d2c222a15dc8fb/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37249c68cc1b471c970ef5b6331cdb55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b316b2c2f8254d5db18532db6550b6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/92534 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token count: 172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"CarperAI/openai_summarize_comparisons\")\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the dataset and get the lengths\n",
    "tokenized_lengths = dataset[\"train\"].map(lambda examples: {'lengths': len(tokenizer(examples['chosen'], add_special_tokens=True)[\"input_ids\"])}, remove_columns=dataset[\"train\"].column_names)\n",
    "# Fetch max length\n",
    "max_length = max(tokenized_lengths[\"lengths\"])\n",
    "print(\"Max token count:\", max_length)\n",
    "\n",
    "\n",
    "\n",
    "# Shuffle the indices\n",
    "total_samples = len(dataset[\"valid2\"])\n",
    "all_indices = list(range(total_samples))\n",
    "random.shuffle(all_indices)\n",
    "\n",
    "\n",
    "# Select 'n'' random indices\n",
    "n_samples = 40000 # With 12500 samples, train at 80% will be 10k\n",
    "selected_indices = all_indices[:n_samples]\n",
    "\n",
    "# Get the 'n'' random samples\n",
    "dataset = dataset[\"valid2\"].select(selected_indices)\n",
    "\n",
    "len(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Dataset into Train, Validation, and Test Sets\n",
    "\n",
    "- **Setting Proportions**: We've designated `80%` of the dataset for training (`train_percent`), `10%` for validation (`val_percent`), and by default, the remaining `10%` will be reserved for testing. It's worth noting that these percentages are predicated on the entirety of the dataset.\n",
    "\n",
    "- **Computing Dataset Sizes**: We compute the absolute number of samples for the training and validation subsets based on their respective percentages (`train_percent` and `val_percent`) and the total sample count `n_samples`.\n",
    "\n",
    "- **Splitting the Dataset**:\n",
    "  - The **Training Set** consists of the initial `train_size` samples.\n",
    "  - The **Validation Set** starts immediately after the training set and encompasses `val_size` samples.\n",
    "  - The **Test Set** comprises the samples that follow the validation dataset, making up the remainder of `n_samples`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, val, and test\n",
    "train_percent = 0.8\n",
    "val_percent = 0.1\n",
    "# test_percent is implicitly 0.1 since train + val + test = 1.0\n",
    "\n",
    "train_size = int(train_percent * n_samples)\n",
    "val_size = int(val_percent * n_samples)\n",
    "# Remaining samples are for testing\n",
    "\n",
    "train_dataset = dataset.select(list(range(train_size)))\n",
    "val_dataset = dataset.select(list(range(train_size, train_size + val_size)))\n",
    "test_dataset = dataset.select(list(range(train_size + val_size, n_samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HF RewardTraining util expects a very specific dataset format with 2 features: chosen and rejected. The dataset we are using includes 'prompt' features. Lets drop it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'prompt' column from each dataset\n",
    "train_dataset = train_dataset.remove_columns(['prompt'])\n",
    "val_dataset = val_dataset.remove_columns(['prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Dataset Cleaning\n",
    "\n",
    "- **Function Definition (`process_features`)**:\n",
    "  - **Tokenization of 'chosen' Feature**: The 'chosen' field of the dataset is tokenized to a maximum length of `512` tokens. It employs padding to achieve a consistent length across all samples. The tokenized `input_ids` and `attention_mask` are added to the dataset under the keys `input_ids_chosen` and `attention_mask_chosen` respectively.\n",
    "  \n",
    "  - **Tokenization of 'rejected' Feature**: Similarly, the 'rejected' field undergoes tokenization, and the results are stored under `input_ids_rejected` and `attention_mask_rejected`.\n",
    "\n",
    "- **Application of Tokenization Function**:\n",
    "  - The `process_features` function is batch-applied to both the `train_dataset` and `val_dataset` using the `map` method. This enriches the datasets with tokenized versions of the 'chosen' and 'rejected' fields.\n",
    "\n",
    "- **Dataset Cleaning**:\n",
    "  - Post tokenization, the original text columns (`chosen` and `rejected`) are superfluous and are thus removed from both `train_dataset` and `val_dataset` to economize on memory and clarity.\n",
    "\n",
    "Upon completion, both datasets are streamlined with tokenized inputs ready for modeling tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each final dataset object should contain two 4 entries:\n",
    "\n",
    "* input_ids_chosen\n",
    "* attention_mask_chosen\n",
    "* input_ids_rejected\n",
    "* attention_mask_rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce236081941d4eaf89345dd558f1992a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c66d225c9564053b614a6ec957b66b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_features(batch):\n",
    "    # Tokenize 'chosen' feature\n",
    "    chosen_tokens = tokenizer(batch['chosen'], padding='max_length', truncation=True, max_length=512, return_tensors='np')\n",
    "    batch['input_ids_chosen'] = chosen_tokens['input_ids']\n",
    "    batch['attention_mask_chosen'] = chosen_tokens['attention_mask']\n",
    "    \n",
    "    # Tokenize 'rejected' feature\n",
    "    rejected_tokens = tokenizer(batch['rejected'], padding='max_length', truncation=True, max_length=512, return_tensors='np')\n",
    "    batch['input_ids_rejected'] = rejected_tokens['input_ids']\n",
    "    batch['attention_mask_rejected'] = rejected_tokens['attention_mask']\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# Apply the function to your datasets\n",
    "train_dataset = train_dataset.map(process_features, batched=True)\n",
    "val_dataset = val_dataset.map(process_features, batched=True)\n",
    "\n",
    "# Remove original 'chosen' and 'rejected' columns\n",
    "columns_to_remove = ['chosen', 'rejected']\n",
    "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
    "val_dataset = val_dataset.remove_columns(columns_to_remove)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the training objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documentation: Metric Loading and Calculation\n",
    "\n",
    "- **Loading the Accuracy Metric**: The `datasets` library provides a suite of metrics for evaluation. Here, we specifically load the \"accuracy\" metric to assess model performance.\n",
    "\n",
    "- **Function Definition (`compute_metrics`)**:\n",
    "  - **Inputs**: The function receives `eval_pred`, a tuple consisting of model predictions (`logits`) and ground truth labels (`labels`).\n",
    "  - **Predictions Extraction**: Given the multi-dimensional `logits` array, we employ `np.argmax` on the last axis to retrieve the index (or class) with the highest predicted score for each sample.\n",
    "  - **Accuracy Computation**: Leveraging the loaded `metric`, we compute the accuracy by comparing the extracted predictions against the true labels (`references`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documentation: Configuration for LoRA (Low-Rank Adaptation)\n",
    "\n",
    "- **Task Type (`task_type`)**: Specifies the nature of the task. Here, it's set to `SEQ_CLS`, indicating a sequence classification task, which is typical for problems where an entire sequence of tokens needs to be classified into a category.\n",
    "\n",
    "- **Inference Mode (`inference_mode`)**: Determines if the configuration is set for inference. It's currently set to `False`, implying that the model is in training or evaluation mode.\n",
    "\n",
    "- **Reduction Rank (`r`)**: Defines the rank for low-rank adaptation. A value of `8` indicates a relatively low rank and can capture essential patterns while ensuring reduced complexity.\n",
    "\n",
    "- **LoRA Alpha (`lora_alpha`)**: An amplification factor. Here, it's set to `32`, which is a hyperparameter that can affect the strength of the low-rank adaptation.\n",
    "\n",
    "- **LoRA Dropout (`lora_dropout`)**: Specifies the dropout rate for LoRA layers. A value of `0.2` suggests that 20% of the inputs will be set to zero, aiding in regularization and potentially preventing overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documentation: Training Arguments Configuration\n",
    "\n",
    "- **Output Directory (`output_dir`)**: Specifies the path where model checkpoints and other training outputs will be saved. In this instance, they will be stored in a directory named `model_bert_hf_experiment2`.\n",
    "\n",
    "- **Training Indication (`do_train`)**: A flag indicating whether to train the model. When set to `True`, the model will undergo training using the provided data.\n",
    "\n",
    "- **Evaluation Indication (`do_eval`)**: A flag that signals if the model should be evaluated. With `True`, the model will be evaluated on the evaluation dataset after training.\n",
    "\n",
    "- **Evaluation Strategy (`evaluation_strategy`)**: Determines the frequency of evaluation during training. The value `\"epoch\"` means that the model will be evaluated at the end of each epoch.\n",
    "\n",
    "- **Saving Strategy (`save_strategy`)**: Dictates when the model checkpoints are saved. Similarly to the evaluation strategy, setting it to `\"epoch\"` ensures model checkpoints are saved after every epoch.\n",
    "\n",
    "- **Number of Training Epochs (`num_train_epochs`)**: Specifies the total number of times the training dataset will be passed through. In this configuration, the training data will be used to train the model for a total of `8` epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './rlhf_reward_model' \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documentation: RewardTrainer Instantiation\n",
    "\n",
    "**RewardTrainer**: An extended trainer class tailored for reinforcement learning with human feedback. \n",
    "\n",
    "- **Model (`model`)**: Specifies the underlying model to be trained. Typically, this will be a pre-trained model like BERT which will be fine-tuned with the given datasets.\n",
    "\n",
    "- **Training Arguments (`args`)**: Contains essential parameters for the training process. This includes the output directory, training/evaluation flags, saving strategy, number of epochs, etc.\n",
    "\n",
    "- **Tokenizer (`tokenizer`)**: Specifies the tokenizer to be used for encoding the input text. This tokenizer will convert text into format suitable for model input.\n",
    "\n",
    "- **Training Dataset (`train_dataset`)**: The primary dataset used to train the model. \n",
    "\n",
    "- **Evaluation Dataset (`eval_dataset`)**: The dataset on which the model will be evaluated to check its performance after training.\n",
    "\n",
    "- **LoRA Configuration (`peft_config`)**: The configuration for the LoRA (Layer-wise Relevance Analysis) technique, which is used for interpreting model decisions and understanding their reasoning.\n",
    "\n",
    "- **Metrics Function (`compute_metrics`)**: Function that will compute metrics like accuracy, based on the model's outputs and the true labels. This is used to gauge the performance of the model.\n",
    "\n",
    "- **Maximum Sequence Length (`max_length`)**: Determines the maximum number of tokens in the input sequence. In this instance, sequences will be truncated or padded to a length of `256` tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\trl\\trainer\\reward_trainer.py:138: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    compute_metrics=compute_metrics,\n",
    "    max_length=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868fa600b3b342fdb71f0752c97ec673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "c:\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2411: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6882, 'learning_rate': 4.921875e-05, 'epoch': 0.12}\n",
      "{'loss': 0.6562, 'learning_rate': 4.8437500000000005e-05, 'epoch': 0.25}\n",
      "{'loss': 0.6447, 'learning_rate': 4.765625e-05, 'epoch': 0.38}\n",
      "{'loss': 0.6406, 'learning_rate': 4.6875e-05, 'epoch': 0.5}\n",
      "{'loss': 0.6336, 'learning_rate': 4.609375e-05, 'epoch': 0.62}\n",
      "{'loss': 0.6388, 'learning_rate': 4.5312500000000004e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6351, 'learning_rate': 4.453125e-05, 'epoch': 0.88}\n",
      "{'loss': 0.6185, 'learning_rate': 4.375e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b429cc59554d4f8784b4be0f0988f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6250253915786743, 'eval_accuracy': 0.64075, 'eval_runtime': 143.3438, 'eval_samples_per_second': 27.905, 'eval_steps_per_second': 3.488, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2411: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6172, 'learning_rate': 4.2968750000000004e-05, 'epoch': 1.12}\n",
      "{'loss': 0.6278, 'learning_rate': 4.21875e-05, 'epoch': 1.25}\n",
      "{'loss': 0.6267, 'learning_rate': 4.140625e-05, 'epoch': 1.38}\n",
      "{'loss': 0.6274, 'learning_rate': 4.0625000000000005e-05, 'epoch': 1.5}\n",
      "{'loss': 0.6248, 'learning_rate': 3.984375e-05, 'epoch': 1.62}\n",
      "{'loss': 0.6078, 'learning_rate': 3.90625e-05, 'epoch': 1.75}\n",
      "{'loss': 0.6249, 'learning_rate': 3.828125e-05, 'epoch': 1.88}\n",
      "{'loss': 0.6157, 'learning_rate': 3.7500000000000003e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797b214d255d47e49363213679aa3b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6143164038658142, 'eval_accuracy': 0.6605, 'eval_runtime': 143.6905, 'eval_samples_per_second': 27.838, 'eval_steps_per_second': 3.48, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2411: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6155, 'learning_rate': 3.671875e-05, 'epoch': 2.12}\n",
      "{'loss': 0.619, 'learning_rate': 3.59375e-05, 'epoch': 2.25}\n",
      "{'loss': 0.6107, 'learning_rate': 3.5156250000000004e-05, 'epoch': 2.38}\n",
      "{'loss': 0.6087, 'learning_rate': 3.4375e-05, 'epoch': 2.5}\n",
      "{'loss': 0.6066, 'learning_rate': 3.359375e-05, 'epoch': 2.62}\n",
      "{'loss': 0.6075, 'learning_rate': 3.2812500000000005e-05, 'epoch': 2.75}\n",
      "{'loss': 0.5972, 'learning_rate': 3.203125e-05, 'epoch': 2.88}\n",
      "{'loss': 0.6139, 'learning_rate': 3.125e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6c93a3102f4a39899bd9cac05861e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6110466122627258, 'eval_accuracy': 0.66475, 'eval_runtime': 142.2232, 'eval_samples_per_second': 28.125, 'eval_steps_per_second': 3.516, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2411: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6055, 'learning_rate': 3.0468750000000002e-05, 'epoch': 3.12}\n",
      "{'loss': 0.6186, 'learning_rate': 2.96875e-05, 'epoch': 3.25}\n",
      "{'loss': 0.6023, 'learning_rate': 2.890625e-05, 'epoch': 3.38}\n",
      "{'loss': 0.5963, 'learning_rate': 2.8125000000000003e-05, 'epoch': 3.5}\n",
      "{'loss': 0.6015, 'learning_rate': 2.734375e-05, 'epoch': 3.62}\n",
      "{'loss': 0.5982, 'learning_rate': 2.6562500000000002e-05, 'epoch': 3.75}\n",
      "{'loss': 0.5977, 'learning_rate': 2.578125e-05, 'epoch': 3.88}\n",
      "{'loss': 0.594, 'learning_rate': 2.5e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160af5906b9b454d8cef2a0ead2c3c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.612758994102478, 'eval_accuracy': 0.6675, 'eval_runtime': 139.6517, 'eval_samples_per_second': 28.643, 'eval_steps_per_second': 3.58, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2411: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5977, 'learning_rate': 2.4218750000000003e-05, 'epoch': 4.12}\n",
      "{'loss': 0.597, 'learning_rate': 2.34375e-05, 'epoch': 4.25}\n",
      "{'loss': 0.5918, 'learning_rate': 2.2656250000000002e-05, 'epoch': 4.38}\n",
      "{'loss': 0.5948, 'learning_rate': 2.1875e-05, 'epoch': 4.5}\n",
      "{'loss': 0.5903, 'learning_rate': 2.109375e-05, 'epoch': 4.62}\n",
      "{'loss': 0.5936, 'learning_rate': 2.0312500000000002e-05, 'epoch': 4.75}\n",
      "{'loss': 0.5831, 'learning_rate': 1.953125e-05, 'epoch': 4.88}\n",
      "{'loss': 0.5947, 'learning_rate': 1.8750000000000002e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55e68a0ecb54c98a4231bd7b2154bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6098501086235046, 'eval_accuracy': 0.66925, 'eval_runtime': 139.5495, 'eval_samples_per_second': 28.664, 'eval_steps_per_second': 3.583, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2411: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5819, 'learning_rate': 1.796875e-05, 'epoch': 5.12}\n",
      "{'loss': 0.5912, 'learning_rate': 1.71875e-05, 'epoch': 5.25}\n",
      "{'loss': 0.582, 'learning_rate': 1.6406250000000002e-05, 'epoch': 5.38}\n",
      "{'loss': 0.576, 'learning_rate': 1.5625e-05, 'epoch': 5.5}\n",
      "{'loss': 0.5764, 'learning_rate': 1.484375e-05, 'epoch': 5.62}\n",
      "{'loss': 0.5853, 'learning_rate': 1.4062500000000001e-05, 'epoch': 5.75}\n",
      "{'loss': 0.5843, 'learning_rate': 1.3281250000000001e-05, 'epoch': 5.88}\n",
      "{'loss': 0.5921, 'learning_rate': 1.25e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02158529610f4e119b04313c761344cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6071081757545471, 'eval_accuracy': 0.67425, 'eval_runtime': 140.8946, 'eval_samples_per_second': 28.39, 'eval_steps_per_second': 3.549, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2411: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5799, 'learning_rate': 1.171875e-05, 'epoch': 6.12}\n",
      "{'loss': 0.5794, 'learning_rate': 1.09375e-05, 'epoch': 6.25}\n",
      "{'loss': 0.5804, 'learning_rate': 1.0156250000000001e-05, 'epoch': 6.38}\n",
      "{'loss': 0.583, 'learning_rate': 9.375000000000001e-06, 'epoch': 6.5}\n",
      "{'loss': 0.5792, 'learning_rate': 8.59375e-06, 'epoch': 6.62}\n",
      "{'loss': 0.5706, 'learning_rate': 7.8125e-06, 'epoch': 6.75}\n",
      "{'loss': 0.5749, 'learning_rate': 7.031250000000001e-06, 'epoch': 6.88}\n",
      "{'loss': 0.585, 'learning_rate': 6.25e-06, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b545f192e4e3413fb436b044ab544bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6101868748664856, 'eval_accuracy': 0.67275, 'eval_runtime': 139.4081, 'eval_samples_per_second': 28.693, 'eval_steps_per_second': 3.587, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2411: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.577, 'learning_rate': 5.46875e-06, 'epoch': 7.12}\n",
      "{'loss': 0.5773, 'learning_rate': 4.6875000000000004e-06, 'epoch': 7.25}\n",
      "{'loss': 0.5711, 'learning_rate': 3.90625e-06, 'epoch': 7.38}\n",
      "{'loss': 0.5765, 'learning_rate': 3.125e-06, 'epoch': 7.5}\n",
      "{'loss': 0.5833, 'learning_rate': 2.3437500000000002e-06, 'epoch': 7.62}\n",
      "{'loss': 0.5594, 'learning_rate': 1.5625e-06, 'epoch': 7.75}\n",
      "{'loss': 0.5822, 'learning_rate': 7.8125e-07, 'epoch': 7.88}\n",
      "{'loss': 0.5703, 'learning_rate': 0.0, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe339dc17aa4c5588ea20f5ef74985a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6113418936729431, 'eval_accuracy': 0.67275, 'eval_runtime': 140.1212, 'eval_samples_per_second': 28.547, 'eval_steps_per_second': 3.568, 'epoch': 8.0}\n",
      "{'train_runtime': 20834.5183, 'train_samples_per_second': 12.287, 'train_steps_per_second': 1.536, 'train_loss': 0.600979121208191, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=32000, training_loss=0.600979121208191, metrics={'train_runtime': 20834.5183, 'train_samples_per_second': 12.287, 'train_steps_per_second': 1.536, 'train_loss': 0.600979121208191, 'epoch': 8.0})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model and Tokenizer\n",
    "\n",
    "After the fine-tuning process, it's crucial to save the model's weights and the tokenizer's configuration for future use, whether it's for inference, further training, or sharing with the community.\n",
    "\n",
    "### 1. Saving the Model\n",
    "\n",
    "To preserve the state of your model post-training, use the `save_pretrained` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model_bert_hf_experiment2/tokenizer_config.json',\n",
       " './model_bert_hf_experiment2/special_tokens_map.json',\n",
       " './model_bert_hf_experiment2/vocab.txt',\n",
       " './model_bert_hf_experiment2/added_tokens.json',\n",
       " './model_bert_hf_experiment2/tokenizer.json')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"./YOUR/PATH/HERE\")\n",
    "tokenizer.save_pretrained(\"./YOUR/PATH/HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using the Fine-tuned Model\n",
    "\n",
    "After saving the fine-tuned model, the next step is to utilize it for generating rewards on sample summaries. The model will produce outputs based on the knowledge it acquired during the fine-tuning process.\n",
    "\n",
    "### Loading the Model\n",
    "\n",
    "To load the model, we will use the `AutoModelForSequenceClassification` class from the `Huggingface` library. This class is tailored for sequence-classification tasks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e0c30c3d884106be8091614ab61fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/678 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59d8b1240ce4462b767a8acd7102552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at JuanKO/rlhf_reward_model were not used when initializing BertForSequenceClassification: ['bert.encoder.layer.5.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.7.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.6.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.5.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.11.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.8.attention.self.query.lora_A.default.weight', 'classifier.modules_to_save.default.bias', 'bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.9.attention.self.query.lora_A.default.weight', 'classifier.modules_to_save.default.weight', 'bert.encoder.layer.5.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.2.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.9.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.2.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.9.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.3.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.10.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.4.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.7.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.10.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.7.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.11.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.10.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.10.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.6.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.8.attention.self.value.lora_A.default.weight', 'classifier.original_module.bias', 'bert.encoder.layer.6.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.8.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.5.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.3.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.6.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.11.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.2.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.3.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.2.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.11.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.4.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.4.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.4.attention.self.value.lora_A.default.weight', 'classifier.original_module.weight', 'bert.encoder.layer.3.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.8.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.9.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.7.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.0.attention.self.query.lora_A.default.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at JuanKO/rlhf_reward_model and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"JuanKO/rlhf_reward_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611b0a8a81f74e298e1d7a7e6c5704f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e6c8458e8d43438aeee2a0513b304f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6993502ea0a4bbab565846320187d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72ba2e162e94a06acb2c857cc17335d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Dropbox\\Desktop\\MAchineLearning\\rlhf\\experiment01\\app_reward_model_hf_lib.ipynb Cell 37\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Dropbox/Desktop/MAchineLearning/rlhf/experiment01/app_reward_model_hf_lib.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mJuanKO/rlhf_reward_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Dropbox/Desktop/MAchineLearning/rlhf/experiment01/app_reward_model_hf_lib.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"JuanKO/rlhf_reward_model\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print metrics\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2411: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983a59571ba744649c2e1e843f349738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions\n",
    "predictions, label_ids, _ = trainer.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.67275\n"
     ]
    }
   ],
   "source": [
    "# Convert logits to labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Compute accuracy or any other metric\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(label_ids, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `score_summaries`\n",
    "\n",
    "### Description:\n",
    "The `score_summaries` function is designed to score two summaries, `chosen_summary` and `rejected_summary`, within the context of a Reinforcement Learning with Human Feedback (RLHF) loop. It tokenizes the inputs, obtains the logits from a given model, computes the softmax probabilities, and finally extracts the scores (probabilities) and logits associated with each summary.\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **model** (`torch.nn.Module`): \n",
    "    - The PyTorch model that produces logits given an input.\n",
    "  \n",
    "- **tokenizer** (`transformers.PreTrainedTokenizer`): \n",
    "    - A tokenizer object used to tokenize input summaries.\n",
    "  \n",
    "- **chosen_summary** (`str`): \n",
    "    - The chosen summary string that needs to be scored.\n",
    "  \n",
    "- **rejected_summary** (`str`): \n",
    "    - The rejected summary string that needs to be scored.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **chosen_score** (`float`): \n",
    "    - The probability score associated with the `chosen_summary` being positive or \"good\".\n",
    "\n",
    "- **rejected_score** (`float`): \n",
    "    - The probability score associated with the `rejected_summary` being positive or \"good\".\n",
    "\n",
    "- **chosen_logit** (`float`): \n",
    "    - The logit value associated with the `chosen_summary`.\n",
    "\n",
    "- **rejected_logit** (`float`): \n",
    "    - The logit value associated with the `rejected_summary`.\n",
    "\n",
    "### Function Flow:\n",
    "\n",
    "1. **Tokenization**: \n",
    "    - The input summaries, `chosen_summary` and `rejected_summary`, are tokenized using the provided tokenizer. These tokenized inputs are padded or truncated to a maximum length of 512 tokens.\n",
    "\n",
    "2. **Move to Device**: \n",
    "    - The tokenized tensors are transferred to the device (likely a GPU or CPU) where the model resides.\n",
    "\n",
    "3. **Obtain Logits**: \n",
    "    - The tokenized tensors are passed through the model to obtain logits. This is done in a no-gradient context to ensure computational efficiency and prevent any updates to the model.\n",
    "\n",
    "4. **Compute Probabilities**: \n",
    "    - The obtained logits are passed through a softmax function to get the associated probabilities. This helps in understanding how likely each summary is deemed \"good\" by the model.\n",
    "\n",
    "5. **Extract Scores and Logits**: \n",
    "    - The function then extracts the probability and logit associated with the positive class (assumed to be the second class in the logits) for both summaries.\n",
    "\n",
    "### Notes:\n",
    "- The function assumes that the positive class (indicating the summary is \"good\") is the second class in the logits.\n",
    "- The softmax function ensures that the logits are converted into probabilities that sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def score_summaries(model, tokenizer, chosen_summary, rejected_summary):\n",
    "    # Tokenize the inputs\n",
    "    chosen_tokens = tokenizer(chosen_summary, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n",
    "    rejected_tokens = tokenizer(rejected_summary, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n",
    "    \n",
    "    chosen_tokens.to(device)\n",
    "    rejected_tokens.to(device)\n",
    "    \n",
    "    # Get logits from the model\n",
    "    with torch.no_grad():\n",
    "        chosen_logits = model(**chosen_tokens).logits\n",
    "        rejected_logits = model(**rejected_tokens).logits\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    chosen_probs = F.softmax(chosen_logits, dim=-1)\n",
    "    rejected_probs = F.softmax(rejected_logits, dim=-1)\n",
    "\n",
    "    # Assuming the positive class (indicating 'chosen' is good) is the second one\n",
    "    chosen_score = chosen_probs[0][1].item()\n",
    "    rejected_score = rejected_probs[0][1].item()\n",
    "    \n",
    "    # Extract logits for each summary\n",
    "    chosen_logit = chosen_logits[0][1].item()\n",
    "    rejected_logit = rejected_logits[0][1].item()\n",
    "\n",
    "    return chosen_score, rejected_score, chosen_logit, rejected_logit\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run sum samples:\n",
    "\n",
    "In this test, we evaluate the `score_summaries` function using two sample summaries: one labeled as `chosen_summary` and the other as `rejected_summary`. These summaries are tokenized, scored, and the associated logits are obtained using our reward model (`rm_model`) and its tokenizer (`rm_tokenizer`).\n",
    "\n",
    "### Sample Summaries:\n",
    "\n",
    "- **Chosen Summary**: \n",
    "    - \"Water meter in another condo is not in our condo. What can we do legally to restore water to my condo complex?\"\n",
    "    \n",
    "- **Rejected Summary**: \n",
    "    - \"Go fix the problem.\"\n",
    "\n",
    "### Test Execution:\n",
    "\n",
    "The `score_summaries` function is called with the provided model, tokenizer, and the sample summaries. The returned scores and logits for each summary are then printed.\n",
    "\n",
    "### Expected Output:\n",
    "\n",
    "- **Chosen Score**: \n",
    "    - This gives the probability score of the `chosen_summary` being perceived as \"good\" or positive by the model.\n",
    "  \n",
    "- **Rejected Score**: \n",
    "    - This gives the probability score of the `rejected_summary` being perceived as \"good\" or positive by the model.\n",
    "  \n",
    "- **Chosen Logit**:\n",
    "    - This returns the raw logit value associated with the `chosen_summary`.\n",
    "  \n",
    "- **Rejected Logit**:\n",
    "    - This returns the raw logit value associated with the `rejected_summary`.\n",
    "\n",
    "### Notes:\n",
    "- Higher scores indicate a higher probability of the summary being perceived as positive or \"good\".\n",
    "- The logit values provide insight into the raw outputs of the model before being passed through the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "chosen_summary = \"TL;DR: Water meter in another condo is not in our condo. What can we do legally to restore water to my condo complex?\"\n",
    "rejected_summary = \"TL;DR: I don't know\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen Score: 0.5634\n",
      "Rejected Score: 0.5950\n",
      "Chosen Logit: 0.1582\n",
      "Rejected Logit: 0.2139\n"
     ]
    }
   ],
   "source": [
    "chosen_score, rejected_score, chosen_logit, rejected_logit = score_summaries(model, tokenizer, chosen_summary, rejected_summary)\n",
    "\n",
    "print(f\"Chosen Score: {chosen_score:.4f}\")\n",
    "print(f\"Rejected Score: {rejected_score:.4f}\")\n",
    "\n",
    "print(f\"Chosen Logit: {chosen_logit:.4f}\")\n",
    "print(f\"Rejected Logit: {rejected_logit:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_on_test_samples(model, tokenizer, test_data, n):\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        chosen_summary = test_data['chosen'][i]\n",
    "        rejected_summary = test_data['rejected'][i]\n",
    "        \n",
    "        chosen_score, rejected_score, chosen_logit, rejected_logit = score_summaries(model, tokenizer, chosen_summary, rejected_summary)\n",
    "        results.append({\n",
    "            'chosen_summary': chosen_summary,\n",
    "            'rejected_summary': rejected_summary,\n",
    "            'chosen_score': chosen_score,\n",
    "            'rejected_score': rejected_score,\n",
    "            'chosen_logit': chosen_logit,\n",
    "            'rejected_logit': rejected_logit\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the evaluation on top 'n' samples\n",
    "n = 20  # or any other number up to 2500\n",
    "results = evaluate_on_test_samples(model, tokenizer, test_dataset, n)\n",
    "\n",
    "# Print results\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"Sample {i} - Chosen Logit: {result['chosen_logit']:.4f} | Rejected Logit: {result['rejected_logit']:.4f}\")\n",
    "    #print(f\"Sample {i} - Chosen Score: {result['chosen_score']:.4f} | Chosen Logit: {result['chosen_logit']:.4f} - Rejected Score: {result['rejected_score']:.4f} | Rejected Logit: {result['rejected_logit']:.4f}\")\n",
    "    #print(f\"Sample {i} - Chosen Summary: {result['chosen_summary']} - Score: {result['chosen_score']:.4f} | Logit: {result['chosen_logit']:.4f}\")\n",
    "    #print(f\"Chosen Summary: {result['chosen_summary']}\")\n",
    "    #print(f\"Chosen Score: {result['chosen_score']:.4f} | Logit: {result['chosen_logit']:.4f}\")\n",
    "    #print(f\"Rejected Summary: {result['rejected_summary']} - Rejected Score: {result['rejected_score']:.4f} | Logit: {result['rejected_logit']:.4f}\")\n",
    "    #print(f\"Rejected Score: {result['rejected_score']:.4f} | Logit: {result['rejected_logit']:.4f}\")\n",
    "    #print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a08e07080b41589afcb037bf32d176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ba74243eff447d90d7643f5f619c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/1.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b052c5eea58440c8bd3e0345a89f9dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/JuanKO/rlhf/commit/993219569a62ffa6bf581996516c05ff21d2b6fd', commit_message='Upload model', commit_description='', oid='993219569a62ffa6bf581996516c05ff21d2b6fd', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.model.push_to_hub('JuanKO/rlhf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Recap\n",
    "\n",
    "### **1. Overview**\n",
    "\n",
    "Throughout this notebook, we've delved into the sophisticated yet powerful domain of Reinforcement Learning from Human Feedback (RLHF) with the overarching goal of enhancing the text summarization abilities of a T5 model. This process, fundamentally rooted in reinforcement learning, utilizes human feedback as the primary mechanism to provide the model with insights into its performance, enabling iterative refinement.\n",
    "\n",
    "### **2. The RLHF Process**\n",
    "\n",
    "The RLHF framework operates in stages:\n",
    "\n",
    "- **Initial Training**: The model (in this case, T5) is trained with standard supervised methods to obtain an initial version.\n",
    "- **Comparison Data Collection**: The model's outputs (summaries) are compared against alternatives to gather feedback on which outputs are better or worse.\n",
    "- **Rewards Model Development**: This is where the BERT model comes into play. Using the comparison data, we train BERT to predict rewards for different model outputs, essentially quantifying the quality of the summaries.\n",
    "- **Fine-Tuning with Proximal Policy Optimization**: Using the rewards provided by the BERT-based rewards model, the main model (T5) is fine-tuned using reinforcement learning techniques. This step is iterative, with the model continuously refining its abilities based on the feedback from the rewards model.\n",
    "\n",
    "### **3. Role of the Rewards Model**\n",
    "\n",
    "BERT, serving as the rewards model, is a pivotal component in this setup. Its primary function is to assess the quality of the summaries generated by the main model. By consuming human comparison data, BERT understands and quantifies the subtle nuances that make one summary better than another. This scalar reward then guides the reinforcement learning process, ensuring that the T5 model's improvements are aligned with human preferences.\n",
    "\n",
    "### **4. Final Thoughts**\n",
    "\n",
    "With the convergence of transformer models, human feedback, and reinforcement learning, the RLHF framework presents a promising avenue for model improvement. As we've seen, not only does it allow for more nuanced training but also ensures that the model's development is continuously tethered to human judgments, leading to more reliable and trustworthy AI systems.\n",
    "\n",
    "By leveraging BERT as the rewards model, we ensure a robust mechanism to gauge and guide the T5 model's performance. The journey we've undertaken in this notebook is a testament to the immense potential and adaptability of modern AI methodologies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook developed by [Juan Olano](https://www.linkedin.com/in/juan-olano-b9a330112/) and [Pano Evangeliou](https://www.linkedin.com/in/p-evangeliou/) - Sept.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
